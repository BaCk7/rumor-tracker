{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "from datetime import datetime  \n",
    "from datetime import timedelta  \n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "class Tracker(object):\n",
    "     \n",
    "    def __init__(self, db_name, collection,\n",
    "                     window_size = 1*60, # 1 hour, in minutes\n",
    "                     update_for_first_k = 200, update_for_every_n = 10,\n",
    "                     hashtag_boost = 1.5, decay_window = 1, \n",
    "                     decay_factor_T = 1.0/2500, keep_top_perc = 95,similarity_threshold=0.3, host='localhost'):\n",
    "        \n",
    "        self.db = pymongo.MongoClient(host=host)[db_name]\n",
    "        self.collection = self.db[collection]\n",
    "        self.clusters = []\n",
    "        self.window_size = window_size\n",
    "        self.update_for_first_k = update_for_first_k\n",
    "        self.update_for_every_n = update_for_every_n\n",
    "        self.hashtag_boost = hashtag_boost\n",
    "        self.decay_window = decay_window\n",
    "        self.decay_factor_T = decay_factor_T\n",
    "        self.keep_top_perc = keep_top_perc\n",
    "        self.similarity_threshold = similarity_threshold  \n",
    "        self.discarded=[]\n",
    "        \n",
    "    \n",
    "    def clusterize(self,use_decay=False):\n",
    "        \n",
    "        similarity_threshold = self.__similarity_threshold\n",
    "        \n",
    "        stream = self.stream_from_collection(self.__window_size)\n",
    "        \n",
    "        for window in stream:\n",
    "            #print len(window)\n",
    "            \n",
    "            for i,tweet in enumerate(window):\n",
    "                #print \"Tweet \",  tweet['id']\n",
    "                \n",
    "                if not tweet['entities']: # se non ho entità, non posso lavorarci\n",
    "                    ##\"Discarding\"\n",
    "                    self.discarded.append(tweet)\n",
    "                    continue\n",
    "\n",
    "                if len(self.clusters) == 0:#E' il primo tweet, creo il primo cluster\n",
    "                    #print \"First cluster with tweet id \"+tweet['rumor']+\"-\"+str(tweet['id'])\n",
    "                    tweet['sim'] = 0.0\n",
    "                    self.start_new_cluster(tweet)\n",
    "                    continue\n",
    "                \n",
    "                tweet_vector = self.get_tweet_vector(tweet)\n",
    "                \n",
    "                sim_values=[]\n",
    "                for index, cl in enumerate(self.__clusters):\n",
    "                    \n",
    "                    if not cl.isActive:\n",
    "                        continue\n",
    "                        \n",
    "                    current_time = tweet['date']\n",
    "                    k = timedelta(hours= self.decay_window)#1 )# entità considerate solo per k(decay window) hours\n",
    "                    #print (tn - t0) , k\n",
    "                    \n",
    "                    if (current_time - cl.last_add_time) > k: \n",
    "                        # se non faccio aggiunte da molto tempo,\n",
    "                        #considero chiuso il cluster. Lo ignoro\n",
    "                        #print current_time, cl.last_add_time, (current_time - cl.last_add_time) , k\n",
    "                        cl.isActive=False\n",
    "                        #print \"Deactivated \", cl.cluster_index, \" \" , current_time - cl.last_add_time\n",
    "                        continue\n",
    "                    \n",
    "                    # genero centroide     \n",
    "                    if use_decay:\n",
    "                        \n",
    "                        cluster_vector = cl.get_labeled_decayed_centroid(current_time)\n",
    "                    else:\n",
    "                        cluster_vector = cl.get_labeled_centroid()\n",
    "                    \n",
    "                    s = self.cosine_similarity(tweet_vector,cluster_vector)\n",
    "                    \n",
    "                    sim_values.append((index,s))\n",
    "                #-----end loop\n",
    "                \n",
    "                # ho serie di coppie (index,sim value) , che ordino per similarità.        \n",
    "                s = sorted(sim_values, key=lambda s: s[1],reverse=True) # True ordine decrescente.\n",
    "                \n",
    "                \n",
    "                if len(s)> 0:  # len: how many active clusters (active == had a recent update)         \n",
    "                    best_cluster = s[0][0] \n",
    "                    best_sim = s[0][1] \n",
    "                    tweet['sim'] = best_sim\n",
    "\n",
    "                    if best_sim > similarity_threshold:\n",
    "                        # aggiungi al migliore\n",
    "                        self.add_to_cluster(best_cluster,tweet)\n",
    "                    else:\n",
    "                        # se il migliore è sotto la soglia, serve un nuovo cluster\n",
    "                        index = self.start_new_cluster(tweet)\n",
    "                else:\n",
    "                    # no active clusters:\n",
    "                    tweet['sim'] = 0.0\n",
    "                    index = self.start_new_cluster(tweet)\n",
    "                        \n",
    "            #END WINDOW\n",
    "            if len(window) > 0:\n",
    "                self.clear_entities()\n",
    "\n",
    "    def cosine_similarity(self,tweet,cluster):    \n",
    "        #https://stackoverflow.com/a/18424953\n",
    "        v1= []\n",
    "        v2= []\n",
    "        t = set(tweet.keys())\n",
    "        c = set(cluster.keys())        \n",
    "        all_keys= t.union(c)\n",
    "        for i,k in enumerate(all_keys):\n",
    "            if k in t:\n",
    "                v1.append(tweet[k])\n",
    "            else:\n",
    "                v1.append(0.0)\n",
    "\n",
    "            if k in c:\n",
    "                v2.append(cluster[k])\n",
    "            else:\n",
    "                v2.append(0.0) \n",
    "        \n",
    "        s = np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "        #v1 = v1/np.max(v1)\n",
    "        #v2 = v2/np.max(v2)\n",
    "        #s = np.dot(v1, v2)/(np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "        return s\n",
    "    \n",
    "    def get_tweet_vector(self,tweet):\n",
    "        tweet_vector={}\n",
    "        #tweet_entities = [(x[0].lower(),x[1],x[2]) for x in tweet['entities']]\n",
    "        tweet_entities = [x for x in tweet['entities']]\n",
    "        c = Counter([x[0]for x in tweet_entities])\n",
    "        \n",
    "        for x in tweet_entities:\n",
    "            e = x[0]\n",
    "            etype = x[1]\n",
    "            icf_of_entity = self.icf(e)\n",
    "            count = c[e]\n",
    "            weight = (1 + np.log2(count)) * icf_of_entity\n",
    "            if etype =='#':\n",
    "                 weight = weight * self.__hashtag_boost\n",
    "            tweet_vector[e]= weight\n",
    "        return tweet_vector\n",
    "        #return tweet_vector/np.max(tweet_vector)\n",
    "    \n",
    "    \n",
    "    def start_new_cluster(self,tweet):\n",
    "        index = len(self.__clusters)\n",
    "        c = Cluster(index, update_for_first_k = self.update_for_first_k, update_for_every_n = self.update_for_every_n, \n",
    "                 hashtag_boost= self.hashtag_boost, decay_window = self.decay_window,decay_factor_T = self.decay_factor_T)\n",
    "        c.add_tweet(tweet)\n",
    "        self.__clusters.append(c)\n",
    "        return index\n",
    "        \n",
    "    def add_to_cluster(self,cluster_index,tweet):\n",
    "        self.__clusters[cluster_index].add_tweet(tweet)       \n",
    "        q = len(self.clusters[cluster_index].tweets)\n",
    "        #update every n and only for first k new tweets\n",
    "        if (q < self.__update_for_first_k ) and (q %self.__update_for_every_n  == 0 ):\n",
    "            #print cluster_index,q\n",
    "            self.update_cluster(cluster_index,tweet['date'])\n",
    "        \n",
    "    def resemblance(self,tweet,cluster):\n",
    "        tweet_entities = set([x[0] for x in tweet['entities']])\n",
    "        cluster_entities = cluster.get_entities()\n",
    "        i = tweet_entities.intersection(cluster_entities)\n",
    "        u = tweet_entities.union(cluster_entities)\n",
    "        resemblance = float(len(i))/float(len(u))\n",
    "        return resemblance\n",
    "    \n",
    "    def containment(self,tweet,cluster):\n",
    "        tweet_entities = set([x[0] for x in tweet['entities']])\n",
    "        cluster_entities = cluster.get_entities()\n",
    "        i = tweet_entities.intersection(cluster_entities)\n",
    "        containment = float(len(i))/float(len(tweet_entities))\n",
    "        return containment\n",
    "    \n",
    "    def clear_entities(self):\n",
    "        for cluster in self.__clusters:\n",
    "            if cluster.isActive:\n",
    "                cluster.clear_entities(self.__keep_top_perc)\n",
    "    \n",
    "    #inverse cluster frequency: similar to IDF, icf(e) = total # of clusters / # of clusters that contain e\n",
    "    def icf(self,entity):\n",
    "        count = 0\n",
    "        total = float(len(self.__clusters))\n",
    "        \n",
    "        if total == 0:\n",
    "            return 1\n",
    "        \n",
    "        for cluster in self.__clusters:\n",
    "            has_entity = cluster.hasEntity(entity)\n",
    "            if has_entity:\n",
    "                count +=1\n",
    "        \n",
    "        if count == 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 1 + np.log(total/(count))\n",
    "    \n",
    "    def update_cluster(self,cluster_index, current_time):\n",
    "        #print cl.entities\n",
    "        cl = self.__clusters[cluster_index]\n",
    "        dict_e = {v['index']:e for (e,v) in cl.entities.items()}\n",
    "        for tweet_index in xrange(cl.M.shape[1]):\n",
    "            col= cl.M[:,tweet_index]\n",
    "            #print col\n",
    "            tweet = cl.tweets[tweet_index]\n",
    "            #print tweet['ins_id']\n",
    "            items = [e[0] for e in tweet['entities']]\n",
    "            #print items\n",
    "            counter = Counter(items)\n",
    "            #print counter\n",
    "            # con count fai la tf: con np.log2\n",
    "            for i,row in enumerate(col):\n",
    "                #print i\n",
    "                e = dict_e[i]\n",
    "                #print e\n",
    "                if e in items: # oppure if row >0. Se non è presente nel tweet, row == 0\n",
    "                    #print e\n",
    "                    # OLD TF\n",
    "                    old_tf = cl.M[i,tweet_index]\n",
    "\n",
    "                    # WEIGHT entity,tweet\n",
    "                    icf_of_entity = self.icf(e)\n",
    "                    count = counter[e]\n",
    "                    weight = (1 + np.log2(count)) * icf_of_entity\n",
    "                    #if cl.get_entity_type(e) =='#':\n",
    "                    if cl.entities[e]['type'] =='#':\n",
    "                         weight = weight * self.__hashtag_boost\n",
    "                    #sim\n",
    "                    sim=self.resemblance(tweet,cl)\n",
    "\n",
    "                    cl.M[i,tweet_index] = old_tf +  weight * sim\n",
    "                    cl.entities[e]['last_update'] = current_time\n",
    "\n",
    "            #print col\n",
    "            #print cl.M[:,tweet_index]\n",
    "            #print \"\"\n",
    "            #print \"--------------\"\n",
    "            # per fare il calcolo della similarity usi o rsemblance o containment\n",
    "    \n",
    "    \n",
    "    def stream_from_collection(self, window_size=1*60, selection=None):\n",
    "\n",
    "        start = None\n",
    "        windows = []\n",
    "        tweets_in_this_window = []\n",
    "        \n",
    "        if selection is None:\n",
    "            selection = {} # li prende tutti\n",
    "        else:\n",
    "            selection = selection # es: {\"rumor\":\"obama\"}   \n",
    "        match = {'$match': selection}\n",
    "        sort = {'$sort': {'date': 1}}\n",
    "        pipeline = [match, sort]\n",
    "        for record in self.collection.aggregate(pipeline):#for record in self.collection.find().sort([(\"date\", 1)]):\n",
    "            if not start: \n",
    "                start = record[\"date\"]\n",
    "                end = start + timedelta(minutes = window_size )\n",
    "                tweets_in_this_window = []\n",
    "                #tweets_in_this_window.append(record)\n",
    "\n",
    "            if record['date'] <= end:\n",
    "                tweets_in_this_window.append(record)\n",
    "            else:\n",
    "                # questa window è conclusa. Aggiungo i tweet e svuoto lista\n",
    "                windows.append(tweets_in_this_window)\n",
    "\n",
    "                # sposto avanti la finestra    \n",
    "                start = end\n",
    "                end = end + timedelta(minutes = window_size )\n",
    "                # crea nuova lista che conterrà i tweet per questa finestra\n",
    "                tweets_in_this_window = []\n",
    "\n",
    "                # continuo a creare liste vuote finchè non sono nell'intervallo in cui cade il record\n",
    "                while record['date'] > end:\n",
    "                    # questa window è conclusa. Aggiungo i tweet e svuoto lista\n",
    "                    windows.append(tweets_in_this_window)\n",
    "\n",
    "                    # sposto avanti la finestra    \n",
    "                    start = end\n",
    "                    end = end  + timedelta(minutes = window_size )\n",
    "                    # crea nuova lista\n",
    "                    tweets_in_this_window = []\n",
    "\n",
    "                tweets_in_this_window.append(record)\n",
    "            \n",
    "        #chiudi l'utima window\n",
    "        windows.append(tweets_in_this_window)\n",
    "        return iter(windows)#return windows\n",
    "    \n",
    "    def get_labels(self, cluster_size_gte= 100):\n",
    "        #gt: ground truth rumor di appartenenza\n",
    "        gt_labels=[]\n",
    "        pred_labels=[]\n",
    "        gt_relev_labels = []\n",
    "\n",
    "        for i, cl in enumerate(self.clusters):\n",
    "            if len(cl.tweets)<cluster_size_gte :\n",
    "                continue\n",
    "\n",
    "            for t in cl.tweets:\n",
    "                gt_labels.append(t['rumor'])\n",
    "                pred_labels.append(i)\n",
    "                if t['label'] in ['11','12','13','14']:\n",
    "                     gt_relev_labels.append(1)\n",
    "                else:\n",
    "                    gt_relev_labels.append(int(t['label']))\n",
    "        # es: t in cluster 0: (0, obama). Con purity cerco di capire se la mggioranza di label nel cluster 0 è obama o altro\n",
    "        d = {}\n",
    "        for i,e in enumerate(set(gt_labels)):\n",
    "            d[e]=i\n",
    "        gt_labels_numeric = [d[k] for k in gt_labels ]\n",
    "        \n",
    "        return gt_labels, pred_labels, gt_relev_labels, gt_labels_numeric\n",
    "    \n",
    "    def coverage(self,cluster_size_threshold = 100):\n",
    "        \n",
    "        \"\"\"\n",
    "        Coverage signifies what proportion of all tweets found\n",
    "        their way into our selectioned clusters which have more than 100\n",
    "        tweets.\n",
    "        \"\"\"\n",
    "        x = sum([len(cl.tweets) for cl in self.clusters if len(cl.tweets) >= cluster_size_threshold])\n",
    "        #print x\n",
    "        tot = sum([len(cl.tweets) for cl in self.clusters])\n",
    "        #print tot\n",
    "        try:\n",
    "            return float(x)/tot\n",
    "        except:\n",
    "            print \"No cluster over threshold\"\n",
    "            return 0.0   \n",
    "        \n",
    "    #---------------GETTERS AND SETTERS\n",
    "    \n",
    "    @property\n",
    "    def similarity_threshold(self):\n",
    "        return self.__similarity_threshold\n",
    "\n",
    "    @similarity_threshold.setter\n",
    "    def similarity_threshold(self, similarity_threshold):\n",
    "        self.__similarity_threshold = similarity_threshold\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def clusters(self):\n",
    "        return self.__clusters\n",
    "\n",
    "    @clusters.setter\n",
    "    def clusters(self, clusters):\n",
    "        self.__clusters = clusters\n",
    "        \n",
    "    @property\n",
    "    def window_size(self):\n",
    "        return self.__window_size\n",
    "\n",
    "    @window_size.setter\n",
    "    def window_size(self, window_size):\n",
    "        self.__window_size = window_size\n",
    "        \n",
    "    @property\n",
    "    def update_for_first_k(self):\n",
    "        return self.__update_for_first_k\n",
    "\n",
    "    @update_for_first_k.setter\n",
    "    def update_for_first_k(self, update_for_first_k):\n",
    "        self.__update_for_first_k= update_for_first_k\n",
    "    \n",
    "    @property\n",
    "    def update_for_every_n(self):\n",
    "        return self.__update_for_every_n\n",
    "\n",
    "    @update_for_every_n.setter\n",
    "    def update_for_every_n(self,update_for_every_n):\n",
    "        self.__update_for_every_n= update_for_every_n\n",
    "        \n",
    "    @property\n",
    "    def hashtag_boost(self):\n",
    "        return self.__hashtag_boost\n",
    "\n",
    "    @hashtag_boost.setter\n",
    "    def hashtag_boost(self, hashtag_boost):\n",
    "        self.__hashtag_boost = hashtag_boost\n",
    "        \n",
    "    @property\n",
    "    def decay_window(self):\n",
    "        return self.__decay_window\n",
    "\n",
    "    @decay_window.setter\n",
    "    def decay_window(self, decay_window):\n",
    "        self.__decay_window = decay_window    \n",
    "    \n",
    "    @property\n",
    "    def decay_factor_T(self):\n",
    "        return self.__decay_factor_T\n",
    "\n",
    "    @decay_factor_T.setter\n",
    "    def decay_factor_T(self, decay_factor_T):\n",
    "        self.__decay_factor_T = decay_factor_T   \n",
    "        \n",
    "    @property\n",
    "    def keep_top_perc(self):\n",
    "        return self.__keep_top_perc\n",
    "\n",
    "    @keep_top_perc.setter\n",
    "    def keep_top_perc(self, keep_top_perc):\n",
    "        self.__keep_top_perc = keep_top_perc\n",
    "    \n",
    "    @property\n",
    "    def discarded(self):\n",
    "        return self.__discarded\n",
    "\n",
    "    @discarded.setter\n",
    "    def discarded(self, discarded):\n",
    "        self.__discarded= discarded    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Cluster(object):\n",
    "     \n",
    "    def __init__(self,cluster_index,update_for_first_k = 200, update_for_every_n = 10, \n",
    "                 hashtag_boost= 1.5,decay_window = 1,decay_factor_T = 1.0/2500):\n",
    "        \n",
    "        self.update_for_first_k = update_for_first_k\n",
    "        self.update_for_every_n = update_for_every_n\n",
    "        self.hashtag_boost= hashtag_boost\n",
    "        self.frequencies= Counter()\n",
    "        self.entities = OrderedDict() # {}\n",
    "        self.tweets = []\n",
    "        self.M = np.zeros((len(self.tweets), len(self.entities)))\n",
    "        self.cluster_index = cluster_index\n",
    "        self.decay_window = decay_window\n",
    "        self.decay_factor_T = decay_factor_T\n",
    "        self.last_add_time = None\n",
    "        self.isActive = True\n",
    "        \n",
    "    def add_tweet(self,tweet):\n",
    "        frequencies = self.frequencies\n",
    "        entities = self.entities\n",
    "        tweets= self.tweets\n",
    "        M = self.M\n",
    "        self.__last_add_time=tweet['date']\n",
    "        \n",
    "        tweets.append(tweet)\n",
    "        # Aggiungi colonna per il nuovo tweet i\n",
    "        new_col_vector= [np.zeros(M.shape[0])] # lunghezza della nuova colonna = numero attuale di righe in matrice\n",
    "        new_col_position = M.shape[1] # posizione = corrisponde al numero di colonne ( visto che index parte da zero)\n",
    "        M = np.insert(M,new_col_position,new_col_vector,axis=1)\n",
    "       \n",
    "        j = new_col_position # == Met.shape[1]-1 # ultimo tweet aggiunto \n",
    "        \n",
    "        # Per ogni nuova entità nel tweet appena aggiunto\n",
    "        new_entities = {e[0]:e[1] for e in tweet['entities'] if e[0] not in entities} \n",
    "        \n",
    "        for e, e_type in new_entities.items():\n",
    "            # Aggiungi a entities\n",
    "            #print e\n",
    "            index = len(entities.keys())\n",
    "            entities[e] = {'index':index,'type': e_type, 'last_update':tweet['date'] } \n",
    "            \n",
    "            # Aggiungi una nuova riga per l'entità \n",
    "            new_row = [np.zeros(M.shape[1])] # nuova riga lunga come: il numero di colonne\n",
    "            M = np.append(M,new_row,axis=0)\n",
    "        \n",
    "        #riempo la colonna j del nuovo tweet con le frequenze    \n",
    "        c = Counter([e[0] for e in tweet['entities']])\n",
    "        for e in entities:\n",
    "            index = self.get_index(e) #entities[e]['index']\n",
    "            #print \"Set in M \", e, index, j\n",
    "            M[index][j] = c[e]\n",
    "        \n",
    "        self.__M = M\n",
    "        #print self.__M\n",
    "    \n",
    "    def get_decayed_centroid(self,current_time):\n",
    "        \n",
    "        decayed_M = np.zeros(self.M.shape)\n",
    "        T= self.decay_factor_T\n",
    "        tn = current_time\n",
    "        for e,v in self.entities.items():\n",
    "            t0 = v['last_update']\n",
    "            k = timedelta(hours= self.decay_window)#1 )# entità considerate solo per k(decay window) hours\n",
    "            delta= tn-t0\n",
    "            delta_t = delta.total_seconds()\n",
    "            #print \"time \",delta,delta_t\n",
    "            damp = np.exp(-(delta_t)*T)\n",
    "            i = v['index']\n",
    "            new_row = self.__M[i] * damp\n",
    "            #print \"orig \",self.M[i]\n",
    "            #print \"decayed \",new_row\n",
    "            decayed_M[i] = new_row\n",
    "        \n",
    "        #print \"decayedM\",decayed_M\n",
    "        norm_M= np.where(np.max(decayed_M, axis=0)==0, decayed_M, 0.5 + 0.5*decayed_M/np.max(decayed_M, axis=0)) \n",
    "        #print \"NormM\" , norm_M\n",
    "        \n",
    "        ## BOOST PER GLI HASHTAG\n",
    "        # boosted tf entitity = boost x normalized tf\n",
    "        hashtag_indexes = [ v['index'] for e,v in self.__entities.items() if v['type'] == '#']\n",
    "        #print hashtag_indexes\n",
    "        # boost a ogni riga in lista: moltiplicando per self.hashtag_boost\n",
    "        for i in hashtag_indexes:\n",
    "            #print norm_M[i]\n",
    "            norm_M[i] = norm_M[i] * self.__hashtag_boost # * 1.5\n",
    "        \n",
    "        norm_centroid = [np.mean(r) for r in norm_M]\n",
    "        return norm_centroid\n",
    "    \n",
    "    def get_centroid(self):\n",
    "        #METODO 1: normalizzo le frequenze di ogni doc/vettore dividendo per il max nella colonna.\n",
    "        # poi faccio la media\n",
    "        #https://stackoverflow.com/questions/21870727/python-divide-values-in-cell-by-max-in-each-column \n",
    "        A = self.__M \n",
    "        #np.max(a, axis=0) # max of each column\n",
    "        # controllo se max == 0 : avviene quando ho eliminato tutte le entità del tweet\n",
    "        norm_M= np.where(np.max(A, axis=0)==0, A, 0.5 + 0.5*A/np.max(A, axis=0)) \n",
    "        #print norm_M\n",
    "        \n",
    "        ## BOOST PER GLI HASHTAG\n",
    "        # boosted tf entitity = boost x normalized tf\n",
    "        hashtag_indexes = [ v['index'] for e,v in self.__entities.items() if v['type'] == '#']\n",
    "        #print hashtag_indexes\n",
    "        # boost a ogni riga in lista: moltiplicando per self.hashtag_boost\n",
    "        for i in hashtag_indexes:\n",
    "            #print norm_M[i]\n",
    "            norm_M[i] = norm_M[i] * self.__hashtag_boost # * 1.5\n",
    "        \n",
    "        norm_centroid = [np.mean(r) for r in norm_M]\n",
    "        return norm_centroid\n",
    "    \n",
    "    def get_labeled_centroid(self):       \n",
    "        \n",
    "        norm_centroid = self.get_centroid()\n",
    "        c = {}\n",
    "        for e in self.__entities:\n",
    "            index = self.__entities[e]['index']\n",
    "            c[e] = norm_centroid[index]\n",
    "        return c\n",
    "    \n",
    "    def get_labeled_decayed_centroid(self,current_time):       \n",
    "\n",
    "        norm_centroid = self.get_decayed_centroid(current_time)\n",
    "        #print norm_centroid\n",
    "        c = {}\n",
    "        for e in self.__entities:\n",
    "            index = self.__entities[e]['index']\n",
    "            c[e] = norm_centroid[index]\n",
    "        return c\n",
    "        \n",
    "    def get_index(self,entity):\n",
    "        try:\n",
    "            index = self.__entities[entity]['index']\n",
    "            return index\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    def get_entity_type(self,entity):\n",
    "        etype = c.entities[entity]['type']\n",
    "        return etype\n",
    "    \n",
    "    def hasEntity(self,entity):\n",
    "        if entity in self.__entities: #d = {'a': 1, 'b': 2} 'a' in d True\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "          \n",
    "    def get_entities(self):\n",
    "        return self.__entities.keys()\n",
    "    \n",
    "    def clear_entities(self,perc=95):\n",
    "    \n",
    "        b = [ sum(r) for r in self.__M ]\n",
    "        #print b\n",
    "        try:\n",
    "            p = np.percentile(b,perc)\n",
    "            #print p\n",
    "            to_del = np.where(b<p)[0]\n",
    "            to_del = sorted(to_del,reverse=True)\n",
    "            dict_e = {v['index']:e for (e,v) in self.__entities.items()}\n",
    "            realign = dict_e.values()\n",
    "            #print self.__M.shape\n",
    "            for k in to_del:\n",
    "                #recupera stringa\n",
    "                e = dict_e[k]\n",
    "                #print k,e\n",
    "\n",
    "                #rimuovi da entities\n",
    "                del self.__entities[e]\n",
    "\n",
    "                # rimuovi dalla M\n",
    "                self.__M =np.delete(self.__M,[k],0)\n",
    "\n",
    "                # rimuovi da array di supporto realign.\n",
    "                realign.remove(e)\n",
    "\n",
    "            # alla fine del ciclo, in realign trovi le entità col nuovo ordine\n",
    "            # vai a sistemare index per ogni entity\n",
    "            for new_index,e in enumerate(realign):\n",
    "                self.__entities[e]['index']= new_index    \n",
    "        \n",
    "            #print self.__M.shape\n",
    "            \n",
    "        except Exception as e: \n",
    "            print b\n",
    "            print(e)\n",
    "            \n",
    "            \n",
    "#-----------------------GETTERS, SETTERS----------------------------------      \n",
    "\n",
    "    @property\n",
    "    def cluster_index(self):\n",
    "        return self.__cluster_index\n",
    "\n",
    "    @cluster_index.setter\n",
    "    def cluster_index(self, cluster_index):\n",
    "        self.__cluster_index = cluster_index\n",
    "    \n",
    "    @property\n",
    "    def entities(self):\n",
    "        return self.__entities\n",
    "\n",
    "    @entities.setter\n",
    "    def entities(self, entities):\n",
    "        self.__entities = entities\n",
    "        \n",
    "    @property\n",
    "    def M(self):\n",
    "        return self.__M\n",
    "\n",
    "    @M.setter\n",
    "    def M(self, M):\n",
    "        self.__M = M\n",
    "        \n",
    "    @property\n",
    "    def frequencies(self):\n",
    "        return self.__frequencies\n",
    "\n",
    "    @frequencies.setter\n",
    "    def frequencies(self, frequencies):\n",
    "        self.__frequencies = frequencies\n",
    "        \n",
    "    @property\n",
    "    def tweets(self):\n",
    "        return self.__tweets\n",
    "\n",
    "    @tweets.setter\n",
    "    def tweets(self, tweets):\n",
    "        self.__tweets= tweets  \n",
    "   \n",
    "    @property\n",
    "    def update_for_first_k(self):\n",
    "        return self.__update_for_first_k\n",
    "\n",
    "    @update_for_first_k.setter\n",
    "    def update_for_first_k(self, update_for_first_k):\n",
    "        self.__update_for_first_k= update_for_first_k\n",
    "    \n",
    "    @property\n",
    "    def update_for_every_n(self):\n",
    "        return self.__update_for_every_n\n",
    "\n",
    "    @update_for_every_n.setter\n",
    "    def update_for_every_n(self, update_for_every_n):\n",
    "        self.__update_for_every_n= update_for_every_n\n",
    "        \n",
    "    @property\n",
    "    def hashtag_boost(self):\n",
    "        return self.__hashtag_boost\n",
    "\n",
    "    @hashtag_boost.setter\n",
    "    def hashtag_boost(self, hashtag_boost):\n",
    "        self.__hashtag_boost = hashtag_boost\n",
    "        \n",
    "    @property\n",
    "    def decay_window(self):\n",
    "        return self.__decay_window\n",
    "\n",
    "    @decay_window.setter\n",
    "    def decay_window(self, decay_window):\n",
    "        self.__decay_window = decay_window    \n",
    "    \n",
    "    @property\n",
    "    def decay_factor_T(self):\n",
    "        return self.__decay_factor_T\n",
    "\n",
    "    @decay_factor_T.setter\n",
    "    def decay_factor_T(self, decay_factor_T):\n",
    "        self.__decay_factor_T = decay_factor_T   \n",
    "        \n",
    "    @property\n",
    "    def last_add_time(self):\n",
    "        return self.__last_add_time\n",
    "\n",
    "    @last_add_time.setter\n",
    "    def last_add_time(self, last_add_time):\n",
    "        self.__last_add_time = last_add_time \n",
    "        \n",
    "    @property\n",
    "    def isActive(self):\n",
    "        return self.__isActive\n",
    "\n",
    "    @isActive.setter\n",
    "    def isActive(self, isActive):\n",
    "        self.__isActive = isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "jd = Tracker('inforet','rumors_entity_filtered', similarity_threshold=0.4, keep_top_perc=50,decay_window= 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3600\n",
      "200\n",
      "10\n",
      "1.5\n",
      "24\n",
      "0.0004\n",
      "50\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print jd.window_size\n",
    "print jd.update_for_first_k\n",
    "print jd.update_for_every_n\n",
    "print jd.hashtag_boost\n",
    "print jd.decay_window\n",
    "print jd.decay_factor_T\n",
    "print jd.keep_top_perc\n",
    "#print jd.icfs\n",
    "print jd.clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "a = datetime.datetime.now()\n",
    "print \"start \", a\n",
    "\n",
    "jd.clusterize(use_decay=True)\n",
    "b = datetime.datetime.now()\n",
    "\n",
    "print \"end \", b\n",
    "print \"delta\", b-a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1188\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print len(jd.clusters)\n",
    "print len(jd.discarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16L, 1L)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl = jd.clusters[2]\n",
    "cl.M.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27nb]",
   "language": "python",
   "name": "conda-env-py27nb-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
